# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15HXQ6V0CVYsBLmvmtF8GsdeaxKvf9LuH
"""

# mount from drive
from google.colab import drive
drive.mount('/content/drive')

#import important library
import os
from langchain.embeddings import HuggingFaceEmbeddings
import pandas as pd
import torch
from langchain.document_loaders import DataFrameLoader
from langchain import HuggingFaceHub
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"
import mishkal.tashkeel

#read fehres data
fehrs = pd.read_csv("/content/drive/MyDrive/Quran_QA/DataCsv/fehres.csv")

#important function
def shkl (text):
  vocalizer = mishkal.tashkeel.TashkeelClass()
  text_after=vocalizer.tashkeel(text)
  return text_after

def answer_question(message, context):
    #load model
    state_dict = torch.load('/content/drive/MyDrive/Quran_QA/presentation/result_just_train/quranqa_model/pytorch_model.bin', map_location=torch.device('cpu'))
    model = AutoModelForQuestionAnswering.from_pretrained("Damith/AraELECTRA-discriminator-QuranQA")
    model.load_state_dict(state_dict)

    pipe = pipeline("question-answering", model=model, tokenizer="Damith/AraELECTRA-discriminator-QuranQA")
    result_row = pipe(question=message,context=context,handle_impossible_answer=True, topk=1)
    return result_row["answer"]

def predict(question):
  # for question and contexts embeddings
  embeddings = HuggingFaceEmbeddings(model_name = "symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli")
  db = FAISS.load_local("/content/drive/MyDrive/Quran_QA/my_index.faiss",embeddings)

  # load large language model
  llm = HuggingFaceHub(repo_id = "google/flan-t5-base")
  retriever = db.as_retriever()

  # retrive over contexts
  qa = RetrievalQA.from_chain_type(
      llm=llm,
      chain_type="map_reduce",
      retriever=retriever,
      return_source_documents=True,)

  # choose first passage to extract answer
  result = qa({"query": question})
  context =""
  answer = ""
  for i in range(3):
    context = result["source_documents"][i].page_content
    # get answer
    context_name = result["source_documents"][0].metadata["pq_id"]
    surah = context_name.split(":")[0]
    surah_name = surah_name = fehrs[fehrs["0"]== int(surah)]["1"].values[0]
    answer += shkl(answer_question(question, context)) +"(سورة" +surah_name +")."
  return answer